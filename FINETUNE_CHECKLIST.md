# GR00T Fine-tuning ì²´í¬ë¦¬ìŠ¤íŠ¸ ë° ê°€ì´ë“œ

## ğŸ¯ ìŠ¤í¬ë¦½íŠ¸ ì‘ë™ ì›ë¦¬

### ì „ì²´ ì›Œí¬í”Œë¡œìš°
```
1. SLURMì´ 4-GPU ë…¸ë“œ í• ë‹¹
2. ë…¸ë“œì˜ ë¹ ë¥¸ scratch ê³µê°„($TMPDIR)ìœ¼ë¡œ í”„ë¡œì íŠ¸ ë³µì‚¬
3. Hugging Faceì—ì„œ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ (scratchì— ì§ì ‘)
4. Python í™˜ê²½ ë° CUDA ë¡œë“œ
5. 4-GPU ë¶„ì‚° í•™ìŠµìœ¼ë¡œ fine-tuning ì‹¤í–‰
6. ì™„ë£Œ í›„ ì²´í¬í¬ì¸íŠ¸ë¥¼ home ë””ë ‰í† ë¦¬ë¡œ ë³µì‚¬
```

### ì£¼ìš” ì¥ì 
- âœ… **ë¹ ë¥¸ I/O**: scratch ê³µê°„ ì‚¬ìš©ìœ¼ë¡œ ë°ì´í„° ë¡œë”© ì†ë„ í–¥ìƒ
- âœ… **ê³µê°„ ì ˆì•½**: home ë””ë ‰í† ë¦¬ í• ë‹¹ëŸ‰ ì ˆì•½
- âœ… **ìë™ ì •ë¦¬**: job ì¢…ë£Œ ì‹œ scratch ìë™ ì‚­ì œ
- âœ… **ë©€í‹° GPU**: 4-GPU ë³‘ë ¬ ì²˜ë¦¬ë¡œ í•™ìŠµ ì†ë„ 4ë°°

---

## âœ… ì‚¬ì „ í™•ì¸ ì²´í¬ë¦¬ìŠ¤íŠ¸

### 1ë‹¨ê³„: ê¸°ë³¸ í™˜ê²½ í…ŒìŠ¤íŠ¸
```bash
cd ~/Isaac-GR00T
./test_setup.sh
```

**í™•ì¸ í•­ëª©:**
- [ ] Isaac-GR00T ë””ë ‰í† ë¦¬ ì¡´ì¬
- [ ] `logs/` ë””ë ‰í† ë¦¬ ì¡´ì¬ (ìë™ ìƒì„±ë¨)
- [ ] `scripts/gr00t_finetune.py` íŒŒì¼ ì¡´ì¬
- [ ] ê°€ìƒí™˜ê²½ `gr00t/` ì¡´ì¬ ë° í™œì„±í™” ê°€ëŠ¥
- [ ] PyTorch, torchvision, huggingface_hub ì„¤ì¹˜ë¨
- [ ] `hf` ëª…ë ¹ì–´ ì‚¬ìš© ê°€ëŠ¥

### 2ë‹¨ê³„: SLURM í™˜ê²½ í…ŒìŠ¤íŠ¸ (30ë¶„ ì†Œìš”)
```bash
# logs ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±
mkdir -p ~/Isaac-GR00T/logs

# í…ŒìŠ¤íŠ¸ job ì œì¶œ
sbatch test_finetune_dryrun.slurm
```

**í™•ì¸ ë°©ë²•:**
```bash
# Job ìƒíƒœ í™•ì¸
squeue -u $USER

# Job ID í™•ì¸ í›„ ë¡œê·¸ ë³´ê¸°
tail -f ~/Isaac-GR00T/logs/test_<JOB_ID>.out
```

**ì´ í…ŒìŠ¤íŠ¸ê°€ í™•ì¸í•˜ëŠ” ê²ƒ:**
- [ ] SLURM jobì´ ì •ìƒì ìœ¼ë¡œ ì‹œì‘ë¨
- [ ] $TMPDIR ì‚¬ìš© ê°€ëŠ¥ ë° ì¶©ë¶„í•œ ê³µê°„
- [ ] í”„ë¡œì íŠ¸ ë³µì‚¬ ì„±ê³µ
- [ ] Python ëª¨ë“ˆ ë¡œë“œ ì„±ê³µ
- [ ] GPU ì¸ì‹ ë° CUDA ì‘ë™
- [ ] Hugging Face CLI ì‘ë™
- [ ] rsyncë¡œ ê²°ê³¼ ë³µì‚¬ ì„±ê³µ

---

## ğŸš€ ì‹¤ì œ Fine-tuning ì‹¤í–‰

### ì‹¤í–‰ ì „ ìµœì¢… í™•ì¸
```bash
# 1. ê°€ìƒí™˜ê²½ì— í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜ í™•ì¸
source ~/Isaac-GR00T/gr00t/bin/activate
pip list | grep -E 'torch|huggingface'

# 2. Hugging Face ë¡œê·¸ì¸ (ì²˜ìŒ í•œ ë²ˆë§Œ)
huggingface-cli login

# 3. logs ë””ë ‰í† ë¦¬ í™•ì¸
ls -ld ~/Isaac-GR00T/logs
```

### Job ì œì¶œ
```bash
cd ~/Isaac-GR00T
sbatch finetune_gr00t.slurm
```

ì œì¶œ í›„ Job IDê°€ í‘œì‹œë©ë‹ˆë‹¤ (ì˜ˆ: `Submitted batch job 12345`)

---

## ğŸ“Š í•™ìŠµ ëª¨ë‹ˆí„°ë§

### ë°©ë²• 1: ëª¨ë‹ˆí„°ë§ ìŠ¤í¬ë¦½íŠ¸ ì‚¬ìš©
```bash
# Job IDë¥¼ í™•ì¸ í›„ ì‹¤í–‰
./monitor_training.sh <JOB_ID>

# ì˜ˆì‹œ:
./monitor_training.sh 12345
```

**ì œê³µ ì •ë³´:**
- Job ìƒíƒœ (PENDING, RUNNING, COMPLETED)
- ì‹¤í–‰ ë…¸ë“œ ë° GPU ì‚¬ìš©ë¥ 
- ë¡œê·¸ íŒŒì¼ ìƒíƒœ
- í•™ìŠµ ì§„í–‰ë¥  (step, loss)
- ì—ëŸ¬ ë°œìƒ ì—¬ë¶€
- ì²´í¬í¬ì¸íŠ¸ ì €ì¥ í˜„í™©

### ë°©ë²• 2: ìˆ˜ë™ ëª¨ë‹ˆí„°ë§

#### Job ìƒíƒœ í™•ì¸
```bash
# ë‚´ ëª¨ë“  Job ë³´ê¸°
squeue -u $USER

# íŠ¹ì • Job ìƒì„¸ ì •ë³´
scontrol show job <JOB_ID>
```

**ìƒíƒœ ì„¤ëª…:**
- `PENDING (PD)`: ëŒ€ê¸° ì¤‘ (ìì› í• ë‹¹ ëŒ€ê¸°)
- `RUNNING (R)`: ì‹¤í–‰ ì¤‘
- `COMPLETING (CG)`: ì¢…ë£Œ ì¤‘
- `COMPLETED (CD)`: ì™„ë£Œ
- `FAILED (F)`: ì‹¤íŒ¨

#### ì‹¤ì‹œê°„ ë¡œê·¸ ë³´ê¸°
```bash
# Output ë¡œê·¸ (í•™ìŠµ ì§„í–‰ ìƒí™©)
tail -f ~/Isaac-GR00T/logs/finetune_<JOB_ID>.out

# Error ë¡œê·¸ (ë¬¸ì œ ë°œìƒ ì‹œ)
tail -f ~/Isaac-GR00T/logs/finetune_<JOB_ID>.err
```

#### GPU ì‚¬ìš©ë¥  í™•ì¸
```bash
# ì‹¤ì‹œê°„ GPU ëª¨ë‹ˆí„°ë§
watch -n 2 "srun --jobid=<JOB_ID> nvidia-smi"
```

**ì •ìƒ ìƒíƒœ:**
- GPU Utilization: 80-100%
- Memory Usage: ê±°ì˜ ì „ë¶€ ì‚¬ìš©
- Temperature: 60-85Â°C

---

## ğŸ” ê° ë‹¨ê³„ë³„ ì˜ˆìƒ ì‹œê°„ ë° ë¡œê·¸

### 1. ì´ˆê¸°í™” ë‹¨ê³„ (2-5ë¶„)
**ë¡œê·¸ ë‚´ìš©:**
```
========================================
Job ID: 12345
Node: gpu-node-01
Working directory: /tmp/slurm.12345.0
========================================
Copying project to scratch...
```

**í™•ì¸:**
- Job IDì™€ ë…¸ë“œ í• ë‹¹ í™•ì¸
- í”„ë¡œì íŠ¸ ë³µì‚¬ ì„±ê³µ

### 2. ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ (10-30ë¶„)
**ë¡œê·¸ ë‚´ìš©:**
```
Downloading datasets to scratch...
Downloading dataset 1/3: recode-bimanual-red-block-basket-v2.1...
Downloading: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.2G/1.2G
```

**í™•ì¸:**
- 3ê°œ ë°ì´í„°ì…‹ ëª¨ë‘ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ
- "Datasets downloaded successfully!" ë©”ì‹œì§€

### 3. í™˜ê²½ ì„¤ì • (1-2ë¶„)
**ë¡œê·¸ ë‚´ìš©:**
```
========================================
Environment Info:
Python: Python 3.10.8
PyTorch: 2.x.x
CUDA available: True
GPU count: 4
GPU 0: NVIDIA A100-SXM4-80GB
GPU 1: NVIDIA A100-SXM4-80GB
GPU 2: NVIDIA A100-SXM4-80GB
GPU 3: NVIDIA A100-SXM4-80GB
========================================
```

**í™•ì¸:**
- 4ê°œ GPU ëª¨ë‘ ì¸ì‹
- CUDA ì‚¬ìš© ê°€ëŠ¥

### 4. Fine-tuning ì‹¤í–‰ (ìˆ˜ ì‹œê°„ ~ 20ì‹œê°„)
**ë¡œê·¸ ë‚´ìš©:**
```
Starting fine-tuning...
Step 0/10000 | Loss: 2.345 | LR: 0.0001
Step 100/10000 | Loss: 1.234 | LR: 0.0001
...
```

**í™•ì¸:**
- Step ë²ˆí˜¸ ì¦ê°€
- Loss ê°’ ê°ì†Œ ì¶”ì„¸
- GPU ì‚¬ìš©ë¥  ë†’ìŒ

**ì˜ˆìƒ ì‹œê°„:**
- 10,000 steps
- Stepë‹¹ ì•½ 3-5ì´ˆ (4 GPU ê¸°ì¤€)
- ì´ 8-14ì‹œê°„ ì •ë„

### 5. ê²°ê³¼ ë³µì‚¬ (5-10ë¶„)
**ë¡œê·¸ ë‚´ìš©:**
```
Copying results back to home directory...
sending incremental file list
checkpoint-1000/
checkpoint-1000/pytorch_model.bin
...
Training completed successfully!
Checkpoints saved to: /home/jxl2244/Isaac-GR00T/so101-bimanual-checkpoints/
```

**í™•ì¸:**
- ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ë“¤ì´ homeìœ¼ë¡œ ë³µì‚¬ë¨
- "Training completed successfully!" ë©”ì‹œì§€

---

## âš ï¸ ë¬¸ì œ í•´ê²°

### ë¬¸ì œ 1: Jobì´ PENDING ìƒíƒœì—ì„œ ë©ˆì¶¤
**ì›ì¸:** ìì› ë¶€ì¡± (4-GPU ë…¸ë“œ ëŒ€ê¸° ì¤‘)

**í™•ì¸:**
```bash
squeue -u $USER -o "%.18i %.9P %.8T %.10M %.10l %.6D %R"
```

**í•´ê²°:**
- `Reason` ì»¬ëŸ¼ í™•ì¸
- `Priority`: ìš°ì„ ìˆœìœ„ ëŒ€ê¸° â†’ ê¸°ë‹¤ë¦¼
- `Resources`: ìì› ë¶€ì¡± â†’ ì‹œê°„ëŒ€ ë³€ê²½ ë˜ëŠ” GPU ê°œìˆ˜ ì¤„ì´ê¸°

### ë¬¸ì œ 2: "hf: command not found"
**ì›ì¸:** Hugging Face CLI ë¯¸ì„¤ì¹˜

**í•´ê²°:**
```bash
source ~/Isaac-GR00T/gr00t/bin/activate
pip install -U "huggingface_hub[cli]"
```

### ë¬¸ì œ 3: ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨
**ì›ì¸:** Hugging Face ì¸ì¦ í•„ìš”

**í•´ê²°:**
```bash
# ë¡œê·¸ì¸ ë…¸ë“œì—ì„œ
source ~/Isaac-GR00T/gr00t/bin/activate
huggingface-cli login
# Token ì…ë ¥ (https://huggingface.co/settings/tokens)
```

### ë¬¸ì œ 4: GPU Out of Memory (OOM)
**ì›ì¸:** Batch sizeê°€ GPU ë©”ëª¨ë¦¬ë³´ë‹¤ í¼

**í•´ê²° (finetune_gr00t.slurm ìˆ˜ì •):**
```bash
# Line 93-94 ìˆ˜ì •
  --batch-size 4 \                           # 8 â†’ 4ë¡œ ì¤„ì„
  --gradient-accumulation-steps 8 \          # 4 â†’ 8ë¡œ ëŠ˜ë¦¼ (íš¨ê³¼ì  batch size ìœ ì§€)
```

### ë¬¸ì œ 5: í•™ìŠµì´ ë©ˆì¶˜ ê²ƒ ê°™ìŒ
**í™•ì¸:**
```bash
# ë¡œê·¸ íŒŒì¼ ë§ˆì§€ë§‰ ìˆ˜ì • ì‹œê°„ í™•ì¸
ls -lh ~/Isaac-GR00T/logs/finetune_<JOB_ID>.out

# ìµœê·¼ 10ë¶„ ë‚´ ì—…ë°ì´íŠ¸ í™•ì¸
find ~/Isaac-GR00T/logs/ -name "finetune_*.out" -mmin -10

# GPUê°€ ì‘ë™ ì¤‘ì¸ì§€ í™•ì¸
srun --jobid=<JOB_ID> nvidia-smi
```

**ì •ìƒ:** GPU ì‚¬ìš©ë¥  80-100%, ë¡œê·¸ íŒŒì¼ì´ ê³„ì† ì—…ë°ì´íŠ¸ë¨  
**ë¬¸ì œ:** GPU ì‚¬ìš©ë¥  0%, ë¡œê·¸ ë©ˆì¶¤ â†’ Job ì·¨ì†Œ í›„ ì¬ì‹œì‘

### ë¬¸ì œ 6: Jobì´ ë„ˆë¬´ ì˜¤ë˜ ê±¸ë¦¼
**í™•ì¸:**
```bash
# í˜„ì¬ step í™•ì¸
grep -i "step" ~/Isaac-GR00T/logs/finetune_<JOB_ID>.out | tail -5

# Stepë‹¹ ì‹œê°„ ê³„ì‚°
# ì˜ˆ: Step 100ê¹Œì§€ 500ì´ˆ = 5ì´ˆ/step
# 10,000 steps Ã— 5ì´ˆ = 50,000ì´ˆ = ì•½ 14ì‹œê°„
```

**ì°¸ê³ :** 24ì‹œê°„ ì œí•œ ë‚´ì— ì™„ë£Œë˜ë„ë¡ `--max-steps` ì¡°ì • ê°€ëŠ¥

---

## ğŸ’¾ ê²°ê³¼ í™•ì¸

### ì²´í¬í¬ì¸íŠ¸ ìœ„ì¹˜
```bash
ls -lh ~/Isaac-GR00T/so101-bimanual-checkpoints/
```

**ì˜ˆìƒ íŒŒì¼:**
```
checkpoint-1000/
checkpoint-2000/
...
checkpoint-10000/
final_model/
```

### ì²´í¬í¬ì¸íŠ¸ í¬ê¸°
- ê° ì²´í¬í¬ì¸íŠ¸: ì•½ 5-10GB
- ì „ì²´: 50-100GB (10ê°œ ì²´í¬í¬ì¸íŠ¸ ê¸°ì¤€)

### ë¡œê·¸ ë³´ê´€
```bash
# ë¡œê·¸ íŒŒì¼ ìœ„ì¹˜
~/Isaac-GR00T/logs/finetune_<JOB_ID>.out
~/Isaac-GR00T/logs/finetune_<JOB_ID>.err
```

---

## ğŸ“ ë¦¬ì†ŒìŠ¤ ì‚¬ìš© í†µê³„ í™•ì¸

### Job ì™„ë£Œ í›„
```bash
# ìƒì„¸ í†µê³„ í™•ì¸
sacct -j <JOB_ID> --format=JobID,JobName,Partition,State,Elapsed,CPUTime,MaxRSS,MaxVMSize

# GPU í†µê³„ (ìˆëŠ” ê²½ìš°)
sacct -j <JOB_ID> --format=JobID,State,Elapsed,ReqGRES,AllocGRES
```

---

## ğŸ”„ ë‹¤ìŒ ì‹¤í–‰ ì‹œ ê°œì„ 

### 1. ì´ë¯¸ ë‹¤ìš´ë¡œë“œí•œ ë°ì´í„°ì…‹ ì¬ì‚¬ìš©
ìŠ¤í¬ë¦½íŠ¸ë¥¼ ìˆ˜ì •í•˜ì—¬ home ë””ë ‰í† ë¦¬ì— ë°ì´í„°ì…‹ ìºì‹œ:

```bash
# finetune_gr00t.slurmì˜ 33-50í–‰ ìˆ˜ì •
# ì²« ì‹¤í–‰: ë‹¤ìš´ë¡œë“œ í›„ homeì— ì €ì¥
# ì´í›„ ì‹¤í–‰: homeì—ì„œ scratchë¡œ ë³µì‚¬ (ë‹¤ìš´ë¡œë“œë³´ë‹¤ ë¹ ë¦„)

if [ -d "$HOME/Isaac-GR00T/demo_data_cache" ]; then
    echo "Copying cached datasets from home..."
    cp -r $HOME/Isaac-GR00T/demo_data_cache ./demo_data
else
    echo "Downloading datasets..."
    # ... ê¸°ì¡´ ë‹¤ìš´ë¡œë“œ ì½”ë“œ ...
    # ë‹¤ìš´ë¡œë“œ í›„ ìºì‹œ ì €ì¥
    cp -r ./demo_data $HOME/Isaac-GR00T/demo_data_cache
fi
```

### 2. ì²´í¬í¬ì¸íŠ¸ ì¤‘ê°„ ì €ì¥
ê¸´ í•™ìŠµ ì‹œ ì¤‘ê°„ ì²´í¬í¬ì¸íŠ¸ë¥¼ ì£¼ê¸°ì ìœ¼ë¡œ homeìœ¼ë¡œ ë³µì‚¬:

```bash
# í•™ìŠµ ì¤‘ ë°±ê·¸ë¼ìš´ë“œë¡œ ì£¼ê¸°ì  ë³µì‚¬ (ì„ íƒì )
# fine-tuning ìŠ¤í¬ë¦½íŠ¸ì—ì„œ --save-steps 1000 ì„¤ì • í™•ì¸
```

---

## ğŸ“ ì¶”ê°€ ë„ì›€

### HPC ì§€ì› ì„¼í„°
- Email: hpc-support@your-institution.edu
- ë¬¸ì„œ: https://hpc.your-institution.edu/docs

### ìœ ìš©í•œ SLURM ëª…ë ¹ì–´
```bash
squeue -u $USER           # ë‚´ Job ëª©ë¡
scontrol show job <ID>    # Job ìƒì„¸ ì •ë³´
scancel <ID>              # Job ì·¨ì†Œ
sinfo -p markov_gpu       # íŒŒí‹°ì…˜ ìƒíƒœ
seff <ID>                 # Job íš¨ìœ¨ì„± ë¶„ì„ (ì™„ë£Œ í›„)
```

---

## âœ¨ ìš”ì•½

**ì‹¤í–‰ ìˆœì„œ:**
1. `./test_setup.sh` â†’ ê¸°ë³¸ í™˜ê²½ í™•ì¸
2. `sbatch test_finetune_dryrun.slurm` â†’ SLURM í™˜ê²½ í…ŒìŠ¤íŠ¸
3. `sbatch finetune_gr00t.slurm` â†’ ì‹¤ì œ fine-tuning
4. `./monitor_training.sh <JOB_ID>` â†’ ì§„í–‰ ìƒí™© ëª¨ë‹ˆí„°ë§

**ì •ìƒ ì‘ë™ ì‹œ íƒ€ì„ë¼ì¸:**
- 0-5ë¶„: ì´ˆê¸°í™” ë° í”„ë¡œì íŠ¸ ë³µì‚¬
- 5-35ë¶„: ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ
- 35ë¶„-14ì‹œê°„: Fine-tuning ì‹¤í–‰
- 14-14.5ì‹œê°„: ê²°ê³¼ ë³µì‚¬
- ì™„ë£Œ!

**ë¬¸ì œ ë°œìƒ ì‹œ:**
- Error ë¡œê·¸ í™•ì¸: `tail ~/Isaac-GR00T/logs/finetune_<JOB_ID>.err`
- ì´ ë¬¸ì„œì˜ "ë¬¸ì œ í•´ê²°" ì„¹ì…˜ ì°¸ì¡°
- HPC ì§€ì› ì„¼í„° ë¬¸ì˜

Good luck! ğŸš€

