# ğŸš€ Isaac-GR00T: ìµœê³  ì„±ëŠ¥ í•™ìŠµ ì „ëµ

## ğŸ“Š ì„œë²„ GPU ì„±ëŠ¥ ë¶„ì„

### **ì‚¬ìš© ê°€ëŠ¥í•œ GPU ëª©ë¡**

| GPU ëª¨ë¸ | ê°œìˆ˜ | VRAM | FP32 TFLOPS | FP16 TFLOPS | ë…¸ë“œ | ê°€ìš©ì„± |
|----------|------|------|-------------|-------------|------|--------|
| **RTX 4090** | 4 | 24GB | 82.6 | 165.2 | classt25 | âš ï¸ ê²½ìŸ ë†’ìŒ |
| **H100 NVL** | 2 | 94GB | 51.2 | 1979 | classt23-24 | âš ï¸ MIG ëª¨ë“œ (11GB) |
| **L40S** | 6Ã—2 | 48GB | 91.6 | 183.2 | classt21-22 | âœ… ëŒ€ê¸° ê°€ëŠ¥ |
| **RTX 2080 Ti** | 4Ã—15 | 11GB | 13.4 | 26.9 | classt01-18 | âœ… ë§ì´ ê°€ìš© |
| **RTX 4070** | 4 | 12GB | 29.1 | 58.2 | classt06 | âœ… ê°€ìš© |

### **ì„±ëŠ¥ ìˆœìœ„ (FP16 ê¸°ì¤€ - AI í•™ìŠµ ìµœì )**

```
1. ğŸ¥‡ H100 NVL: 1979 TFLOPS (í•˜ì§€ë§Œ MIGë¡œ 11GBë§Œ ì‚¬ìš© ê°€ëŠ¥)
2. ğŸ¥ˆ L40S: 183.2 TFLOPS Ã— 6 = 1099 TFLOPS (ë‹¨ì¼ ë…¸ë“œ)
3. ğŸ¥‰ RTX 4090: 165.2 TFLOPS Ã— 4 = 660 TFLOPS (ë‹¨ì¼ ë…¸ë“œ)
4. RTX 4070: 58.2 TFLOPS Ã— 4 = 232 TFLOPS
5. RTX 2080 Ti: 26.9 TFLOPS Ã— 4 = 107 TFLOPS
```

---

## ğŸ¯ ìµœì  í•™ìŠµ ì „ëµ

### **Strategy 1: ì¦‰ì‹œ ì‹œì‘ (ê¶Œì¥ â­)**

**ëª©í‘œ**: ëŒ€ê¸° ì‹œê°„ ìµœì†Œí™”, ë¹ ë¥¸ ë°˜ë³µ

```bash
ë…¸ë“œ: classt25 (RTX 4090 Ã— 4)
ì„±ëŠ¥: 660 TFLOPS (FP16)
ë©”ëª¨ë¦¬: 96GB ì´
Batch size: 8-12 per GPU
ëŒ€ê¸° ì‹œê°„: ë‚®ìŒ-ì¤‘ê°„
```

**ì„¤ì •:**
```bash
#SBATCH --gres=gpu:gpu4090:4
#SBATCH --nodelist=classt25
--batch-size 10
--gradient-accumulation-steps 3
# Effective: 10 Ã— 4 Ã— 3 = 120
```

**ì˜ˆìƒ ì‹œê°„**: 10,000 steps â‰ˆ 10-12ì‹œê°„

---

### **Strategy 2: ìµœê³  ì„±ëŠ¥ (6 GPU)**

**ëª©í‘œ**: ìµœëŒ€ ì²˜ë¦¬ëŸ‰, ë¹ ë¥¸ ì™„ë£Œ

```bash
ë…¸ë“œ: classt21 ë˜ëŠ” classt22 (L40S Ã— 6)
ì„±ëŠ¥: 1099 TFLOPS (FP16)
ë©”ëª¨ë¦¬: 288GB ì´
Batch size: 10-14 per GPU
ëŒ€ê¸° ì‹œê°„: ì¤‘ê°„-ë†’ìŒ
```

**ì„¤ì •:**
```bash
#SBATCH --gres=gpu:gpul40s:6
#SBATCH --nodelist=classt21
--batch-size 12
--gradient-accumulation-steps 2
# Effective: 12 Ã— 6 Ã— 2 = 144
```

**ì˜ˆìƒ ì‹œê°„**: 10,000 steps â‰ˆ 7-9ì‹œê°„

---

### **Strategy 3: ê²½ì œì  (RTX 2080 Ti)**

**ëª©í‘œ**: í•­ìƒ ì‚¬ìš© ê°€ëŠ¥, ë¹„ìš© íš¨ìœ¨

```bash
ë…¸ë“œ: classt01-18 (RTX 2080 Ti Ã— 4)
ì„±ëŠ¥: 107 TFLOPS (FP16)
ë©”ëª¨ë¦¬: 44GB ì´
Batch size: 4-6 per GPU
ëŒ€ê¸° ì‹œê°„: ê±°ì˜ ì—†ìŒ
```

**ì„¤ì •:**
```bash
#SBATCH --gres=gpu:gpu2080:4
--batch-size 5
--gradient-accumulation-steps 6
# Effective: 5 Ã— 4 Ã— 6 = 120
```

**ì˜ˆìƒ ì‹œê°„**: 10,000 steps â‰ˆ 18-24ì‹œê°„

---

## ğŸ“ Batch Size ìµœì í™” ê³„ì‚°

### **GPU ë©”ëª¨ë¦¬ë³„ ê¶Œì¥ Batch Size**

| GPU | VRAM | ë³´ìˆ˜ì  | ê¶Œì¥ | ê³µê²©ì  | ì£¼ì˜ì‚¬í•­ |
|-----|------|--------|------|--------|----------|
| RTX 2080 Ti | 11GB | 2 | 4 | 6 | OOM ì£¼ì˜ |
| RTX 4070 | 12GB | 3 | 5 | 7 | - |
| RTX 4090 | 24GB | 6 | 10 | 14 | ìµœì  |
| L40S | 48GB | 12 | 16 | 20 | ìµœê³  |
| H100 (MIG) | 11GB | 2 | 4 | 6 | MIG ì œí•œ |

### **ê³„ì‚° ê³µì‹**

```python
# GR00T fine-tuning ë©”ëª¨ë¦¬ ì¶”ì •
base_model_memory = 5 GB  # Model weights
per_sample_memory = 1.2 GB  # Gradients + activations
overhead = 2 GB  # CUDA kernels, etc.

max_batch_size = floor((GPU_VRAM - base_model_memory - overhead) / per_sample_memory)
```

### **Effective Batch Size ìœ ì§€**

```
Target Effective Batch Size: 120-144

Formula: batch_size Ã— num_gpus Ã— gradient_accumulation_steps = 120-144

Examples:
- 4 GPU, batch=10: 10 Ã— 4 Ã— 3 = 120 âœ…
- 6 GPU, batch=12: 12 Ã— 6 Ã— 2 = 144 âœ…
- 4 GPU, batch=5:  5 Ã— 4 Ã— 6 = 120 âœ…
```

---

## âš¡ ì„±ëŠ¥ ìµœì í™” íŒ

### **1. Mixed Precision Training**

```python
# ì´ë¯¸ ìŠ¤í¬ë¦½íŠ¸ì— í¬í•¨ë˜ì–´ ìˆì„ ê°€ëŠ¥ì„± ë†’ìŒ
fp16=True  # 2ë°° ë¹ ë¦„, ë©”ëª¨ë¦¬ ì ˆë°˜
```

### **2. Gradient Checkpointing**

```python
# ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ
gradient_checkpointing=True  # ë©”ëª¨ë¦¬ 50% ê°ì†Œ, ì†ë„ 20% ê°ì†Œ
```

### **3. DataLoader ìµœì í™”**

```python
num_workers=8  # CPU ì½”ì–´ í™œìš©
pin_memory=True  # GPU ì „ì†¡ ë¹ ë¦„
prefetch_factor=2  # ë¯¸ë¦¬ ë¡œë“œ
```

### **4. ì»´íŒŒì¼ ìµœì í™” (PyTorch 2.0+)**

```python
torch.compile()  # 30-40% ì†ë„ í–¥ìƒ (ì´ˆê¸° ì»´íŒŒì¼ ì‹œê°„ í•„ìš”)
```

---

## ğŸ¯ ìµœì¢… ì¶”ì²œ ì „ëµ

### **ìƒí™©ë³„ ìµœì  ì„ íƒ**

#### **ğŸƒ ë¹ ë¥¸ í…ŒìŠ¤íŠ¸/ë””ë²„ê¹…**
```bash
RTX 2080 Ti Ã— 4 (í•­ìƒ ê°€ìš©)
--batch-size 4
--max-steps 100
ì‹¤í–‰ ì‹œê°„: ~30ë¶„
```

#### **âš–ï¸ ê· í˜•ì¡íŒ í•™ìŠµ (ì¶”ì²œ)**
```bash
RTX 4090 Ã— 4 (ëŒ€ê¸° ì‹œê°„ ë‚®ìŒ)
--batch-size 10
--max-steps 10000
ì‹¤í–‰ ì‹œê°„: ~10ì‹œê°„
```

#### **ğŸš€ ìµœê³  ì„±ëŠ¥**
```bash
L40S Ã— 6 (ëŒ€ê¸° ê°€ëŠ¥í•˜ë©´)
--batch-size 12
--max-steps 10000
ì‹¤í–‰ ì‹œê°„: ~7ì‹œê°„
```

---

## ğŸ“‹ ì‹¤í–‰ ì²´í¬ë¦¬ìŠ¤íŠ¸

### **Job ì œì¶œ ì „**

- [ ] ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±: `mkdir -p ~/Isaac-GR00T/logs`
- [ ] Hugging Face ë¡œê·¸ì¸: `huggingface-cli login`
- [ ] GPU ê°€ìš©ì„± í™•ì¸: `sinfo -p markov_gpu`
- [ ] í• ë‹¹ëŸ‰ í™•ì¸: `quota -s`

### **Job ì œì¶œ**

```bash
# Strategy 1: 4090 (ê¶Œì¥)
sbatch finetune_gr00t.slurm

# Strategy 2: L40S (ìµœê³  ì„±ëŠ¥)
sbatch --nodelist=classt21 --gres=gpu:gpul40s:6 finetune_gr00t_l40s.slurm

# Strategy 3: 2080 Ti (í•­ìƒ ê°€ìš©)
sbatch --gres=gpu:gpu2080:4 finetune_gr00t_2080.slurm
```

### **ëª¨ë‹ˆí„°ë§**

```bash
# Job ìƒíƒœ
squeue -u $USER

# ì‹¤ì‹œê°„ ë¡œê·¸
tail -f ~/Isaac-GR00T/logs/finetune_<JOB_ID>.out

# GPU ì‚¬ìš©ë¥ 
srun --jobid=<JOB_ID> nvidia-smi
```

---

## ğŸ’¾ ì €ì¥ ê³µê°„ ê´€ë¦¬

### **ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰**

```bash
Home í• ë‹¹ëŸ‰: 23.8GB
í˜„ì¬ ì‚¬ìš©: ~1GB (ìµœì í™” í›„)

ì˜ˆìƒ ì²´í¬í¬ì¸íŠ¸ í¬ê¸°:
- ê° checkpoint: 5-8GB
- 10ê°œ checkpoint: 50-80GB
- ìµœì¢… ëª¨ë¸: 5-8GB
```

### **ì €ì¥ ì „ëµ**

```bash
# ì²´í¬í¬ì¸íŠ¸ëŠ” scratchì— ì €ì¥ë˜ê³ , ì™„ë£Œ í›„ homeìœ¼ë¡œ ë³µì‚¬ë¨
# ì¤‘ê°„ ì²´í¬í¬ì¸íŠ¸ ì„ íƒì  ì €ì¥ (ê³µê°„ ì ˆì•½)
--save-strategy "steps"
--save-steps 2000  # 5ê°œë§Œ ì €ì¥
```

---

## ğŸ”¬ ì‹¤í—˜ ë¡œê·¸

### **Experiment 1: Baseline**
```
Date: [TBD]
GPUs: RTX 4090 Ã— 4
Batch Size: 8
Steps: 10000
Time: [TBD]
Final Loss: [TBD]
```

### **Experiment 2: Optimized**
```
Date: [TBD]
GPUs: [TBD]
Batch Size: [TBD]
Steps: 10000
Time: [TBD]
Final Loss: [TBD]
```

---

## ğŸ“ ë¬¸ì œ í•´ê²°

### **ì¼ë°˜ì ì¸ ì´ìŠˆ**

1. **OOM (Out of Memory)**
   - Batch sizeë¥¼ ì ˆë°˜ìœ¼ë¡œ ì¤„ì´ê¸°
   - Gradient accumulation 2ë°° ëŠ˜ë¦¬ê¸°
   - Gradient checkpointing í™œì„±í™”

2. **ëŠë¦° ë°ì´í„° ë¡œë”©**
   - `num_workers` ì¦ê°€ (8-16)
   - `prefetch_factor` ì¦ê°€ (2-4)
   - ë°ì´í„°ì…‹ í¬ê¸° í™•ì¸

3. **GPU ì‚¬ìš©ë¥  ë‚®ìŒ (<80%)**
   - Batch size ì¦ê°€
   - DataLoader workers ì¦ê°€
   - ë³‘ëª© ì§€ì  í”„ë¡œíŒŒì¼ë§

4. **Hang/Timeout**
   - NCCL ì„¤ì • í™•ì¸
   - ë„¤íŠ¸ì›Œí¬ ë¬¸ì œ (ë©€í‹° ë…¸ë“œ)
   - Timeout ê°’ ì¦ê°€

---

## ğŸ“ ì°¸ê³  ìë£Œ

- [PyTorch Performance Tuning](https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html)
- [NVIDIA Mixed Precision Training](https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html)
- [Hugging Face Training Tips](https://huggingface.co/docs/transformers/performance)

---

**Created**: 2025-10-25  
**Last Updated**: 2025-10-25  
**Status**: Active  

