#!/bin/bash
#SBATCH --job-name=gr00t_test
#SBATCH --partition=markov_gpu
#SBATCH --gres=gpu:1              # í…ŒìŠ¤íŠ¸ëŠ” 1ê°œ GPUë§Œ ì‚¬ìš©
#SBATCH --cpus-per-task=8
#SBATCH --mem=64G
#SBATCH --time=00:30:00           # 30ë¶„ë§Œ í…ŒìŠ¤íŠ¸
#SBATCH --output=/home/jxl2244/Isaac-GR00T/logs/test_%j.out
#SBATCH --error=/home/jxl2244/Isaac-GR00T/logs/test_%j.err
#SBATCH --nodes=1
#SBATCH --ntasks=1

# Error handling
set -euo pipefail

echo "=========================================="
echo "GR00T Fine-tuning í…ŒìŠ¤íŠ¸ (Dry Run)"
echo "=========================================="
echo ""

# 1. SLURM í™˜ê²½ ì •ë³´
echo "1ï¸âƒ£  SLURM í™˜ê²½ ì •ë³´"
echo "Job ID: $SLURM_JOB_ID"
echo "Node: $SLURMD_NODENAME"
echo "Partition: $SLURM_JOB_PARTITION"
echo "GPUs: $SLURM_GPUS"
echo "CPUs: $SLURM_CPUS_PER_TASK"
echo "Memory: $SLURM_MEM_PER_NODE MB"
echo ""

# 2. ì‘ì—… ë””ë ‰í† ë¦¬ í…ŒìŠ¤íŠ¸
echo "2ï¸âƒ£  ì‘ì—… ë””ë ‰í† ë¦¬ ($TMPDIR) í…ŒìŠ¤íŠ¸"
WORK_DIR="$TMPDIR"
echo "TMPDIR: $WORK_DIR"
mkdir -p "$WORK_DIR"
cd "$WORK_DIR"
echo "ì‚¬ìš© ê°€ëŠ¥í•œ ê³µê°„: $(df -h . | tail -1 | awk '{print $4}')"
echo "í˜„ì¬ ë””ë ‰í† ë¦¬: $(pwd)"
echo ""

# 3. í”„ë¡œì íŠ¸ ë³µì‚¬ í…ŒìŠ¤íŠ¸
echo "3ï¸âƒ£  í”„ë¡œì íŠ¸ ë³µì‚¬ í…ŒìŠ¤íŠ¸"
echo "ë³µì‚¬ ì‹œì‘ ì‹œê°„: $(date)"
time cp -r $HOME/Isaac-GR00T .
cd Isaac-GR00T
echo "ë³µì‚¬ëœ íŒŒì¼ í¬ê¸°: $(du -sh . | cut -f1)"
echo "ìŠ¤í¬ë¦½íŠ¸ ì¡´ì¬ í™•ì¸:"
ls -lh scripts/gr00t_finetune.py 2>/dev/null && echo "âœ… gr00t_finetune.py ì¡´ì¬" || echo "âŒ ìŠ¤í¬ë¦½íŠ¸ ì—†ìŒ"
echo ""

# 4. ëª¨ë“ˆ ë¡œë“œ í…ŒìŠ¤íŠ¸
echo "4ï¸âƒ£  ëª¨ë“ˆ ë¡œë“œ í…ŒìŠ¤íŠ¸"
module purge
module load Python/3.10.8-GCCcore-12.2.0
module load CUDA/12.6.0
echo "âœ… ëª¨ë“ˆ ë¡œë“œ ì™„ë£Œ"
echo ""

# 5. ê°€ìƒí™˜ê²½ í…ŒìŠ¤íŠ¸
echo "5ï¸âƒ£  ê°€ìƒí™˜ê²½ í™œì„±í™” í…ŒìŠ¤íŠ¸"
source $HOME/Isaac-GR00T/gr00t/bin/activate
echo "Python: $(python --version)"
echo "Python ê²½ë¡œ: $(which python)"
echo ""

# 6. PyTorch ë° CUDA í…ŒìŠ¤íŠ¸
echo "6ï¸âƒ£  PyTorch ë° CUDA í…ŒìŠ¤íŠ¸"
python << 'EOF'
import torch
print(f"PyTorch ë²„ì „: {torch.__version__}")
print(f"CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}")
print(f"CUDA ë²„ì „: {torch.version.cuda}")
print(f"GPU ê°œìˆ˜: {torch.cuda.device_count()}")
if torch.cuda.is_available():
    for i in range(torch.cuda.device_count()):
        print(f"GPU {i}: {torch.cuda.get_device_name(i)}")
        mem = torch.cuda.get_device_properties(i).total_memory / 1024**3
        print(f"  ë©”ëª¨ë¦¬: {mem:.1f} GB")
EOF
echo ""

# 7. Hugging Face CLI í…ŒìŠ¤íŠ¸
echo "7ï¸âƒ£  Hugging Face CLI í…ŒìŠ¤íŠ¸"
if command -v hf &> /dev/null; then
    echo "âœ… hf ëª…ë ¹ì–´ ì‚¬ìš© ê°€ëŠ¥"
    hf --version
    echo ""
    echo "ìƒ˜í”Œ ë°ì´í„°ì…‹ ì •ë³´ ê°€ì ¸ì˜¤ê¸° (ë‹¤ìš´ë¡œë“œ ì•ˆí•¨):"
    hf ls --repo-type dataset YieumYoon/recode-bimanual-red-block-basket-v2.1 | head -5 || echo "âš ï¸  ë°ì´í„°ì…‹ ì ‘ê·¼ ì‹¤íŒ¨ (ë¡œê·¸ì¸ í•„ìš”í•  ìˆ˜ ìˆìŒ)"
else
    echo "âŒ hf ëª…ë ¹ì–´ ì—†ìŒ"
    echo "pip install -U huggingface_hub[cli] í•„ìš”"
fi
echo ""

# 8. ì‘ì€ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ í…ŒìŠ¤íŠ¸ (ì„ íƒì )
echo "8ï¸âƒ£  ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ í…ŒìŠ¤íŠ¸ (ìŠ¤í‚µ - ì‹œê°„ ì ˆì•½)"
echo "ì‹¤ì œ SLURM jobì—ì„œ ë‹¤ìš´ë¡œë“œ ë¨"
mkdir -p demo_data
echo ""

# 9. í™˜ê²½ ë³€ìˆ˜ í…ŒìŠ¤íŠ¸
echo "9ï¸âƒ£  GPU í™˜ê²½ ë³€ìˆ˜ í…ŒìŠ¤íŠ¸"
export CUDA_VISIBLE_DEVICES=0
export OMP_NUM_THREADS=8
export NCCL_DEBUG=INFO
export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512

echo "CUDA_VISIBLE_DEVICES: $CUDA_VISIBLE_DEVICES"
echo "OMP_NUM_THREADS: $OMP_NUM_THREADS"
echo "NCCL_DEBUG: $NCCL_DEBUG"
echo "PYTORCH_CUDA_ALLOC_CONF: $PYTORCH_CUDA_ALLOC_CONF"
echo ""

# 10. ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ê°€ëŠ¥ì„± í…ŒìŠ¤íŠ¸ (helpë§Œ)
echo "ğŸ”Ÿ Fine-tuning ìŠ¤í¬ë¦½íŠ¸ í…ŒìŠ¤íŠ¸"
if [ -f "scripts/gr00t_finetune.py" ]; then
    echo "ìŠ¤í¬ë¦½íŠ¸ help ì¶œë ¥:"
    python scripts/gr00t_finetune.py --help || echo "âš ï¸  ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜"
else
    echo "âŒ scripts/gr00t_finetune.py ì—†ìŒ"
fi
echo ""

# 11. ì²´í¬í¬ì¸íŠ¸ ë””ë ‰í† ë¦¬ í…ŒìŠ¤íŠ¸
echo "1ï¸âƒ£1ï¸âƒ£  ì¶œë ¥ ë””ë ‰í† ë¦¬ í…ŒìŠ¤íŠ¸"
mkdir -p checkpoints
mkdir -p logs
echo "âœ… checkpoints ë””ë ‰í† ë¦¬ ìƒì„±ë¨: $(pwd)/checkpoints"
echo "âœ… logs ë””ë ‰í† ë¦¬ ìƒì„±ë¨: $(pwd)/logs"
echo ""

# 12. rsync í…ŒìŠ¤íŠ¸
echo "1ï¸âƒ£2ï¸âƒ£  ê²°ê³¼ ë³µì‚¬ í…ŒìŠ¤íŠ¸"
echo "í…ŒìŠ¤íŠ¸ íŒŒì¼ ìƒì„±..."
echo "test checkpoint" > checkpoints/test.txt
mkdir -p $HOME/Isaac-GR00T/test-checkpoints
rsync -av --progress checkpoints/test.txt $HOME/Isaac-GR00T/test-checkpoints/
if [ -f "$HOME/Isaac-GR00T/test-checkpoints/test.txt" ]; then
    echo "âœ… rsync ì‘ë™ í™•ì¸"
    rm -rf $HOME/Isaac-GR00T/test-checkpoints
else
    echo "âŒ rsync ì‹¤íŒ¨"
fi
echo ""

# ìµœì¢… ìš”ì•½
echo "=========================================="
echo "í…ŒìŠ¤íŠ¸ ì™„ë£Œ!"
echo "=========================================="
echo ""
echo "âœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ê°€ ì„±ê³µí•˜ë©´ ì‹¤ì œ fine-tuning ì‹¤í–‰ ê°€ëŠ¥"
echo ""
echo "ì‹¤ì œ fine-tuning ì‹œì‘:"
echo "  sbatch finetune_gr00t.slurm"
echo ""
echo "Job ëª¨ë‹ˆí„°ë§:"
echo "  squeue -u $USER                                    # Job ìƒíƒœ"
echo "  tail -f ~/Isaac-GR00T/logs/finetune_<JOB_ID>.out  # ì‹¤ì‹œê°„ ë¡œê·¸"
echo "  tail -f ~/Isaac-GR00T/logs/finetune_<JOB_ID>.err  # ì—ëŸ¬ ë¡œê·¸"
echo ""

